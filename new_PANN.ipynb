{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87724992-9ac0-41b8-ba5f-4592c2dad813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path for Training data: /Users/zaher/Desktop/Project/Processed__PANN_Training_Set\n",
      "Path for Validation data : /Users/zaher/Desktop/Project/Processed__PANN_Validation_Set\n",
      "Using device: cpu\n",
      "Loaded files files from /Users/zaher/Desktop/Project/Processed__PANN_Training_Set\n",
      "Loaded files files from /Users/zaher/Desktop/Project/Processed__PANN_Validation_Set\n",
      "Number of training samples: 7573\n",
      "Number of validation samples: 3406\n",
      "Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training: 100%|██████████████████| 237/237 [13:31<00:00,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 0.3441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████| 107/107 [01:35<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0999, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "New best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Training: 100%|██████████████████| 237/237 [13:14<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Training Loss: 0.0880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████| 107/107 [01:35<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0344, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "New best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Training: 100%|██████████████████| 237/237 [13:21<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Training Loss: 0.0393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████| 107/107 [01:40<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0155, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "New best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Training: 100%|██████████████████| 237/237 [13:35<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Training Loss: 0.0231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████| 107/107 [01:39<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0064, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "New best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Training: 100%|██████████████████| 237/237 [14:11<00:00,  3.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Training Loss: 0.0156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████| 107/107 [01:51<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0032, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "New best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Training: 100%|██████████████████| 237/237 [13:22<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Training Loss: 0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████| 107/107 [01:43<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0024, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "New best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Training: 100%|██████████████████| 237/237 [13:59<00:00,  3.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Training Loss: 0.0087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████| 107/107 [01:40<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0016, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "New best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Training: 100%|██████████████████| 237/237 [13:49<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Training Loss: 0.0072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████| 107/107 [01:50<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0011, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "New best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Training: 100%|██████████████████| 237/237 [13:56<00:00,  3.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Training Loss: 0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████| 107/107 [01:41<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "New best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Training: 100%|█████████████████| 237/237 [15:26<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Training Loss: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████| 107/107 [01:45<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "New best model saved.\n",
      "Final evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████| 107/107 [01:37<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0005\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Final model saved\n",
      "Training and evaluation steps completed sucessfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class SimplifiedPANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimplifiedPANN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.dropout(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        return x\n",
    "\n",
    "class TwoLayerCNN(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(TwoLayerCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(16, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.dropout(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class CustomPANNwithCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomPANNwithCNN, self).__init__()\n",
    "        self.pann_extractor = SimplifiedPANN()\n",
    "        self.cnn = TwoLayerCNN(input_channels=1024)  # 64 * 4 * 4 = 1024\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pann_extractor(x)\n",
    "        x = x.view(x.size(0), 1024, 1, 1)\n",
    "        return self.cnn(x)\n",
    "\n",
    "class AugmentedDCASEDataset(Dataset):\n",
    "    def __init__(self, data_path, is_training=True):\n",
    "        self.data_path = data_path\n",
    "        self.is_training = is_training\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "        \n",
    "        if not os.path.exists(data_path):\n",
    "            raise FileNotFoundError(f\"The directory {data_path} does not exist.\")\n",
    "        \n",
    "        for root, _, files in os.walk(data_path):\n",
    "            for file in files:\n",
    "                if file.endswith('.npy'):\n",
    "                    self.files.append(os.path.join(root, file))\n",
    "                    self.labels.append(os.path.basename(root))\n",
    "        \n",
    "        if len(self.files) == 0:\n",
    "            raise ValueError(f\"No .npy files found in {data_path}. Please ensure that you have run the preprocessing step or you have valid files\")\n",
    "        \n",
    "        print(f\"Loaded files files from {data_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.files):\n",
    "            #check for out of range\n",
    "            raise IndexError(f\"index {idx} is out of range, Dataset contains {len(self.files)} files\")\n",
    "        \n",
    "        try:\n",
    "            file_path = self.files[idx]\n",
    "            data = np.load(file_path)\n",
    "            label = self.labels[idx]\n",
    "            \n",
    "            if self.is_training:\n",
    "                data = self.augment(data)\n",
    "            \n",
    "            if len(data.shape) == 1:\n",
    "                data = np.expand_dims(data, axis=1)\n",
    "            elif len(data.shape) > 2:\n",
    "                data = data.reshape(data.shape[0], -1)\n",
    "            \n",
    "            data = np.expand_dims(data, axis=0)\n",
    "            \n",
    "            return torch.FloatTensor(data), torch.FloatTensor([1.0])  \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {file_path}: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "\n",
    "    def augment(self, data):\n",
    "        shift = np.random.randint(-5, 5)\n",
    "        data = np.roll(data, shift, axis=1)\n",
    "        #time shifting frequency masking and randrom noise\n",
    "        if data.shape[0] > 1:\n",
    "            mask_size = np.random.randint(1, min(5, data.shape[0] - 1))\n",
    "            mask_freq = np.random.randint(0, data.shape[0] - mask_size)\n",
    "            data[mask_freq:mask_freq+mask_size, :] = 0\n",
    "        \n",
    "        if data.shape[1] > 1:\n",
    "            mask_size = np.random.randint(1, min(5, data.shape[1] - 1))\n",
    "            mask_time = np.random.randint(0, data.shape[1] - mask_size)\n",
    "            data[:, mask_time:mask_time+mask_size] = 0\n",
    "        \n",
    "        noise = np.random.normal(0, 0.01, data.shape)\n",
    "        data = data + noise\n",
    "        \n",
    "        return data\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return torch.Tensor(), torch.Tensor()\n",
    "    \n",
    "    inputs, labels = zip(*batch)\n",
    "    \n",
    "    max_channels = max(x.size(0) for x in inputs)\n",
    "    max_height = max(x.size(1) for x in inputs)\n",
    "    max_width = max(x.size(2) for x in inputs)\n",
    "    \n",
    "    padded_inputs = []\n",
    "    for x in inputs:\n",
    "        padded = torch.nn.functional.pad(x, (0, max_width - x.size(2), 0, max_height - x.size(1), 0, max_channels - x.size(0)))\n",
    "        padded_inputs.append(padded)\n",
    "    \n",
    "    return torch.stack(padded_inputs), torch.stack(labels)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs, patience=5):\n",
    "    best_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "            if inputs.numel() == 0:\n",
    "                continue\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}\")\n",
    "        #epoch for validation\n",
    "        val_loss, accuracy, precision, recall, f1 = evaluate(model, val_loader, criterion, device)\n",
    "        print(f\"Validation - Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "    #if the results do not improve then do early stop after two epochs\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save(model.state_dict(), 'best_pann_model.pth')\n",
    "            print(\"New best model saved.\")\n",
    "        else:\n",
    "            epochs_without_improvement += 2\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                model.load_state_dict(torch.load('best_pann_model.pth'))\n",
    "                break\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            if inputs.numel() == 0:\n",
    "                continue\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            preds = (outputs > 0.5).float()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    avg_loss = running_loss / len(val_loader)\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_targets, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_targets, all_preds, zero_division=0)\n",
    "    \n",
    "    return avg_loss, accuracy, precision, recall, f1\n",
    "def main():\n",
    "    PROCESSED_TRAIN_PATH = \"/Users/zaher/Desktop/Project/Processed__PANN_Training_Set\"\n",
    "    PROCESSED_VAL_PATH = \"/Users/zaher/Desktop/Project/Processed__PANN_Validation_Set\"\n",
    "\n",
    "    print(f\"Path for Training data: {PROCESSED_TRAIN_PATH}\")\n",
    "    print(f\"Path for Validation data : {PROCESSED_VAL_PATH}\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    try:\n",
    "        train_dataset = AugmentedDCASEDataset(PROCESSED_TRAIN_PATH, is_training=True)\n",
    "        val_dataset = AugmentedDCASEDataset(PROCESSED_VAL_PATH, is_training=False)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Data doesn't exist please make sure that you have run the preprocessing step and check the path of the directories.\")\n",
    "        return\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Data doesn't exist please make sure that you have run the preprocessing step and check the path of the directories.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "    print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model = CustomPANNwithCNN().to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "    num_epochs = 10\n",
    "    print(\"Starting training\")\n",
    "    try:\n",
    "        model = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs)\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered during training: {str(e)}\")\n",
    "\n",
    "    print(\"Final evaluation\")\n",
    "    try:\n",
    "        final_loss, accuracy, precision, recall, f1 = evaluate(model, val_loader, criterion, device)\n",
    "        print(f\"Loss: {final_loss:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "#save the final model\n",
    "        torch.save(model.state_dict(), 'pann_model_final.pth')\n",
    "        print(\"Final model saved\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during final evaluation: {str(e)}\")\n",
    "\n",
    "    print(\"Training and evaluation steps completed sucessfully\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    #this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffac2f9d-97f0-411c-8c33-13a3a489087e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Starting evaluation process\n",
      "Memory usage data: 1321.28 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pg/22n8t2m906vc18bt44y5trf80000gn/T/ipykernel_14831/3820237405.py:215: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.6593\n",
      "Precision: 1.0000\n",
      "Recall: 0.4918\n",
      "predictions saved to pann_predictions.csv\n",
      "Evaluation completed\n",
      "Memory usage data: 1288.08 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import gc\n",
    "import traceback\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "class SimplifiedPANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimplifiedPANN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.dropout(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        return x\n",
    "\n",
    "class TwoLayerCNN(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(TwoLayerCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(16, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.dropout(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class CustomPANNwithCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomPANNwithCNN, self).__init__()\n",
    "        self.pann_extractor = SimplifiedPANN()\n",
    "        self.cnn = TwoLayerCNN(input_channels=1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pann_extractor(x)\n",
    "        x = x.view(x.size(0), 1024, 1, 1)\n",
    "        return self.cnn(x)\n",
    "\n",
    "def print_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    #to check ow much ram we used to tackle the errors\n",
    "    print(f\"Memory usage data: {process.memory_info().rss / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "def load_and_preprocess_file(file_path):\n",
    "    try:\n",
    "        data = np.load(file_path)\n",
    "        #ensuring that the shape of our data is 2d\n",
    "        if len(data.shape) == 1:\n",
    "            data = np.expand_dims(data, axis=0)\n",
    "        elif len(data.shape) > 2:\n",
    "            data = data.reshape(data.shape[0], -1)\n",
    "        \n",
    "        # add dimension if more\n",
    "        data = np.expand_dims(data, axis=0)\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def augment_data(data):\n",
    "    # for time shifting\n",
    "    shift = np.random.randint(-5, 5)\n",
    "    data = np.roll(data, shift, axis=2)\n",
    "    \n",
    "    # for frequency masking\n",
    "    if data.shape[1] > 1:\n",
    "        mask_size = np.random.randint(1, min(5, data.shape[1] - 1))\n",
    "        mask_freq = np.random.randint(0, data.shape[1] - mask_size)\n",
    "        data[:, mask_freq:mask_freq+mask_size, :] = 0\n",
    "    \n",
    "    # for time masking\n",
    "    if data.shape[2] > 1:\n",
    "        mask_size = np.random.randint(1, min(5, data.shape[2] - 1))\n",
    "        mask_time = np.random.randint(0, data.shape[2] - mask_size)\n",
    "        data[:, :, mask_time:mask_time+mask_size] = 0\n",
    "    \n",
    "    # adding a random noise\n",
    "    noise = np.random.normal(0, 0.01, data.shape)\n",
    "    data = data + noise\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_support_and_query_sets(data_path, n_support=5):\n",
    "    class_files = defaultdict(list)\n",
    "    for root, _, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.npy'):\n",
    "                class_name = os.path.basename(root)\n",
    "                class_files[class_name].append(os.path.join(root, file))\n",
    "    support_set = {}\n",
    "    query_set = {}\n",
    "    \n",
    "    for class_name, files in class_files.items():\n",
    "        if len(files) >= n_support:\n",
    "            support_files = random.sample(files, n_support)\n",
    "            query_files = [f for f in files if f not in support_files]\n",
    "            \n",
    "            support_set[class_name] = support_files\n",
    "            query_set[class_name] = query_files\n",
    "    \n",
    "    return support_set, query_set\n",
    "\n",
    "def evaluate_model(model, data_path, device, output_file, n_support=5):\n",
    "    model.eval()\n",
    "    support_set, query_set = create_support_and_query_sets(data_path, n_support)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_file_names = []\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            for class_name, support_files in tqdm(support_set.items(), desc=\"Evaluating classes\"):\n",
    "#create support and query sets\n",
    "                support_features = []\n",
    "                for file in support_files:\n",
    "                    data = load_and_preprocess_file(file)\n",
    "                    if data is not None:\n",
    "                        data = augment_data(data)\n",
    "                        data = torch.FloatTensor(data).unsqueeze(0).to(device)  \n",
    "                        features = model.pann_extractor(data)\n",
    "                        support_features.append(features)\n",
    "                \n",
    "                support_prototype = torch.mean(torch.stack(support_features), dim=0)\n",
    "                \n",
    "#for the query set\n",
    "                for file in query_set[class_name]:\n",
    "                    data = load_and_preprocess_file(file)\n",
    "                    if data is not None:\n",
    "                        data = torch.FloatTensor(data).unsqueeze(0).to(device)  # Add batch dimension\n",
    "                        query_features = model.pann_extractor(data)\n",
    "                        \n",
    "                        distance = torch.norm(query_features - support_prototype)\n",
    "                        prediction = torch.sigmoid(-distance).item()  # Convert distance to probability\n",
    "                        \n",
    "                        all_predictions.append(prediction)\n",
    "                        all_labels.append(1)  \n",
    "                        all_file_names.append(os.path.basename(file))\n",
    "                \n",
    "                del support_features, support_prototype, query_features\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                \n",
    "        \n",
    "     \n",
    "        \n",
    "        threshold = np.median(all_predictions)\n",
    "        \n",
    "   #applying threshpld to get binary classification\n",
    "        binary_predictions = [1 if pred > threshold else 0 for pred in all_predictions]\n",
    "        #calculating our metrics\n",
    "        f1 = f1_score(all_labels, binary_predictions, zero_division=0)\n",
    "        precision = precision_score(all_labels, binary_predictions, zero_division=0)\n",
    "        recall = recall_score(all_labels, binary_predictions, zero_division=0)\n",
    "        \n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        \n",
    "#save results to csv\n",
    "        with open(output_file, 'w', newline='') as csvfile:\n",
    "            fieldnames = ['file_name', 'raw_output', 'binary_prediction']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            \n",
    "            writer.writeheader()\n",
    "            for file_name, raw_output, binary_pred in zip(all_file_names, all_predictions, binary_predictions):\n",
    "                writer.writerow({\n",
    "                    'file_name': file_name, \n",
    "                    'raw_output': raw_output,\n",
    "                    'binary_prediction': binary_pred\n",
    "                })\n",
    "        \n",
    "        print(f\"predictions saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered during evaluation: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "def main():\n",
    "    PROCESSED_EVAL_PATH = \"/Users/zaher/Desktop/Project/Processed__PANN_Evaluation_Set\"\n",
    "    MODEL_PATH = \"best_pann_model.pth\"  \n",
    "    OUTPUT_FILE = \"pann_predictions.csv\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    if not os.path.exists(PROCESSED_EVAL_PATH):\n",
    "        print(f\"Error:  directory {PROCESSED_EVAL_PATH} doesn't  exist.\")\n",
    "        return\n",
    "\n",
    "    model = CustomPANNwithCNN().to(device)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "    \n",
    "    print(\"Starting evaluation process\")\n",
    "    print_memory_usage()\n",
    "    evaluate_model(model, PROCESSED_EVAL_PATH, device, OUTPUT_FILE, n_support=5)\n",
    "    print(\"Evaluation completed\")\n",
    "    print_memory_usage()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277d8929-4c6a-47dd-8b59-76399aa90974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
