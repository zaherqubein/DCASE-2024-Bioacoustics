{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da395a8c-3d76-4cae-8c2f-8cfab3d8a503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "Proccessed data shape: x: (18213, 1, 128, 86), y: (18213,).\n",
      "x dtype: float32, y dtype: int64\n",
      "x range: (0.0, 1.0), y range: (0, 1)\n",
      "Loading the validation data from: /Users/zaher/Desktop/Project/Validation_Set\n",
      "Loaded data from /Users/zaher/Desktop/Project/Validation_Set/val_all.h5\n",
      "Number of validation files: 41\n",
      "Training  path: /Users/zaher/Desktop/Project/Training_Set.\n",
      "Validation  path: /Users/zaher/Desktop/Project/Validation_Set.\n",
      "Training shape: torch.Size([18213, 1, 128, 86]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████████████████████████| 570/570 [35:52<00:00,  3.78s/it]\n",
      "/var/folders/pg/22n8t2m906vc18bt44y5trf80000gn/T/ipykernel_30299/3798208143.py:310: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3983\n",
      "Not enough samples Skipping file: b'BUK1_20181013_023504.wav'.\n",
      "Validation Results:\n",
      "File: b'BUK5_20180921_015906a.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.7500\n",
      "f1: 0.8571\n",
      "\n",
      "File: b'BUK5_20161101_002104a.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9881\n",
      "f1: 0.9940\n",
      "\n",
      "File: b'pw5.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9900\n",
      "f1: 0.9950\n",
      "\n",
      "File: b'pw4.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9947\n",
      "f1: 0.9973\n",
      "\n",
      "File: b'pw9.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9944\n",
      "f1: 0.9972\n",
      "\n",
      "File: b'pw10.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9952\n",
      "f1: 0.9976\n",
      "\n",
      "File: b'pw11.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9933\n",
      "f1: 0.9967\n",
      "\n",
      "File: b'pw8.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9928\n",
      "f1: 0.9964\n",
      "\n",
      "File: b'pw13.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9956\n",
      "f1: 0.9978\n",
      "\n",
      "File: b'pw12.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9877\n",
      "f1: 0.9938\n",
      "\n",
      "File: b'pw15.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9937\n",
      "f1: 0.9968\n",
      "\n",
      "File: b'pw14.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9933\n",
      "f1: 0.9966\n",
      "\n",
      "File: b'BUK1_20181011_001004.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9600\n",
      "f1: 0.9796\n",
      "\n",
      "File: b'BUK5_20180921_015906a.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.8333\n",
      "f1: 0.9091\n",
      "\n",
      "File: b'BUK4_20161031_232104a.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9926\n",
      "f1: 0.9963\n",
      "\n",
      "File: b'BUK5_20181007_011905.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9000\n",
      "f1: 0.9474\n",
      "\n",
      "File: b'R4_cleaned recording_TEL_19-10-17.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9992\n",
      "f1: 0.9996\n",
      "\n",
      "File: b'R4_cleaned recording_13-10-17.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9989\n",
      "f1: 0.9994\n",
      "\n",
      "File: b'file_423_487.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9991\n",
      "f1: 0.9996\n",
      "\n",
      "File: b'R4_cleaned recording_TEL_20-10-17.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9993\n",
      "f1: 0.9996\n",
      "\n",
      "File: b'R4_cleaned recording_TEL_23-10-17.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9993\n",
      "f1: 0.9997\n",
      "\n",
      "File: b'R4_cleaned recording_16-10-17.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9989\n",
      "f1: 0.9995\n",
      "\n",
      "File: b'R4_cleaned recording_TEL_25-10-17.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9995\n",
      "f1: 0.9998\n",
      "\n",
      "File: b'file_97_113.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9981\n",
      "f1: 0.9991\n",
      "\n",
      "File: b'R4_cleaned recording_17-10-17.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9990\n",
      "f1: 0.9995\n",
      "\n",
      "File: b'R4_cleaned recording_TEL_24-10-17.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9994\n",
      "f1: 0.9997\n",
      "\n",
      "File: b'ME1.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9412\n",
      "f1: 0.9697\n",
      "\n",
      "File: b'ME2.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9697\n",
      "f1: 0.9846\n",
      "\n",
      "File: b'RD_06.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9984\n",
      "f1: 0.9992\n",
      "\n",
      "File: b'RD_05.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9971\n",
      "f1: 0.9985\n",
      "\n",
      "File: b'RD_04.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9980\n",
      "f1: 0.9990\n",
      "\n",
      "File: b'RD_01.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9930\n",
      "f1: 0.9965\n",
      "\n",
      "File: b'RD_03.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9231\n",
      "f1: 0.9600\n",
      "\n",
      "File: b'BUK4_20171022_004304a.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9375\n",
      "f1: 0.9677\n",
      "\n",
      "File: b'RD_02.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9989\n",
      "f1: 0.9994\n",
      "\n",
      "File: b'pw1.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9844\n",
      "f1: 0.9921\n",
      "\n",
      "File: b'pw3.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9903\n",
      "f1: 0.9951\n",
      "\n",
      "File: b'pw2.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9929\n",
      "f1: 0.9964\n",
      "\n",
      "File: b'pw6.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9885\n",
      "f1: 0.9942\n",
      "\n",
      "File: b'pw7.wav'\n",
      "precision: 1.0000\n",
      "recall: 0.9919\n",
      "f1: 0.9960\n",
      "\n",
      "Average Results:\n",
      "Precision: 1.0000\n",
      "Recall: 0.9763\n",
      "F1 Score: 0.9873\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import csv\n",
    "\n",
    "#paths to our training/validation \n",
    "train_path = \"/Users/zaher/Desktop/Project/Training_Set\"\n",
    "val_path = \"/Users/zaher/Desktop/Project/Validation_Set\"\n",
    "\n",
    "class AudioResNet(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(AudioResNet, self).__init__()\n",
    "        #set number of input channels to 128\n",
    "        self.in_channels = 128\n",
    "        #first conv network\n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.layer1 = self.make_layer(128, 2, stride=1)\n",
    "        self.layer2 = self.make_layer(256, 2, stride=2)\n",
    "        self.layer3 = self.make_layer(512, 2, stride=2)\n",
    "        self.layer4 = self.make_layer(1024, 2, stride=2)\n",
    "        #reduce the pooling sieze on the feature map to 1 on 1\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "#create a ResNet Layer with multiple Blocks\n",
    "    def make_layer(self, out_channels, blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(self.make_resNET_block(out_channels, stride))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(self.make_resNET_block(out_channels, stride=1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def make_resNET_block(self, out_channels, stride):\n",
    "        downsample = None\n",
    "        #check if we need to do downsample by checking if output channels\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "            #create a ResBlock update number of input channels\n",
    "        block = ResBlock(self.in_channels, out_channels, stride, downsample)\n",
    "        self.in_channels = out_channels\n",
    "        return block\n",
    "        #in this function we pass it to the next layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        #pass the input between our convo layers we have 4\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        #apply pooling\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "    #then pass it through Fully conected lauer amd apply sigmoid activation    \n",
    "        x = self.sigmoid(x)\n",
    "        #then remove any extra dimensions by squeezing it\n",
    "        return x.squeeze()\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avg_pool(x)\n",
    "        #flatten our outpot to get feature vector\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResBlock, self).__init__()\n",
    "        #pass it to the first convo layer\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        #batch normalisation \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return torch.relu(out)\n",
    "\n",
    "def load_Training_data(file_path):\n",
    "    all_segments = []\n",
    "    #to store all features and labels that we extracted in preproccessing\n",
    "    all_labels = []\n",
    "    with h5py.File(file_path, 'r') as hf:\n",
    "        #iterate over each file group in the HDF5 file\n",
    "        for file_group in hf.values():\n",
    "            if isinstance(file_group, h5py.Group):\n",
    "                #load the segments from the current group and append them to thet array\n",
    "                segments = file_group['segments'][:]\n",
    "                labels = file_group['labels'][:]\n",
    "                all_segments.append(segments)\n",
    "                all_labels.append(labels)\n",
    "    #concatenate them into single numpy array\n",
    "    x = np.concatenate(all_segments, axis=0)\n",
    "    y = np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "    print(f\"Proccessed data shape: x: {x.shape}, y: {y.shape}.\")\n",
    "    print(f\"x dtype: {x.dtype}, y dtype: {y.dtype}\")\n",
    "    print(f\"x range: ({x.min()}, {x.max()}), y range: ({y.min()}, {y.max()})\")\n",
    "    #convert numpy into pytorch\n",
    "    return torch.FloatTensor(x), torch.FloatTensor(y)\n",
    "\n",
    "def load_validation_data(val_path):\n",
    "    val_data = []\n",
    "    print(f\"Loading the validation data from: {val_path}\")\n",
    "    \n",
    "    h5_file = os.path.join(val_path, 'val_all.h5')\n",
    "    #check if file exists\n",
    "    if os.path.exists(h5_file):\n",
    "        #open and iterate over the h5 file\n",
    "        with h5py.File(h5_file, 'r') as hf:\n",
    "            for key in hf.keys():\n",
    "                file_data = {'file': hf[key]['file'][()],\n",
    "                    'segments': torch.FloatTensor(hf[key]['segments'][:]),\n",
    "                    'labels': np.array(hf[key]['labels'][:]),\n",
    "                    'start_times': np.array(hf[key]['start_times'][:]),\n",
    "                    'end_times': np.array(hf[key]['end_times'][:])}\n",
    "                #APPEND EACH CREATED DICTIONARY TO VAL_DATA\n",
    "                val_data.append(file_data)\n",
    "        \n",
    "        print(f\"Loaded data from {h5_file}\")\n",
    "        print(f\"Number of validation files: {len(val_data)}\")\n",
    "    else:\n",
    "        print(f\"Couldn't find : {h5_file} file\")\n",
    "    \n",
    "    return val_data\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, scheduler, device, num_epochs=3, patience=5):\n",
    "    model.train()\n",
    "    # set the model to training mode\n",
    "    #generate a maximum value for best loss we willl decrease it later\n",
    "    best_loss = float('inf')\n",
    "    #patience counter helps us to stop early incase our model's performance doesn't improve\n",
    "    patience_counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "#calculate and save the loss in each epoch loop by number of epochs\n",
    "        running_loss = 0.0\n",
    "        #loop over batches from training loader\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            #move to GPU to improve performance and to avoid KERNEL failing\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            #compute loss and update model parameters\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        scheduler.step(epoch_loss)\n",
    "        \n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "#after training the model we validate it using FEW SHOT LEARNING\n",
    "def validate_few_shot(model, val_data, device, thresholds=np.linspace(0, 1, 100), support_size=5, batch_size=16):\n",
    "    model.eval()\n",
    "    all_results = []\n",
    "    #lists to store results and predictions\n",
    "    all_predictions = []\n",
    "\n",
    "    for file_data in val_data:\n",
    "        #loop over validation file\n",
    "        #extract segments,labels start and end time\n",
    "        segments = file_data['segments']\n",
    "        labels = file_data['labels']\n",
    "        start_times = file_data['start_times']\n",
    "        end_times = file_data['end_times']\n",
    "        #find indices of all positive samples\n",
    "        pos_indices = np.where(labels == 1)[0]\n",
    "        #we need at least 5 samples for few shot learning\n",
    "        if len(pos_indices) < support_size:\n",
    "            print(f\"Not enough samples Skipping file: {file_data['file']}.\")\n",
    "            continue\n",
    "        #we select the first 5 positive samples as support set\n",
    "        support_indices = pos_indices[:support_size]\n",
    "        #for the rest of the samples we use them as query\n",
    "        query_indices = np.arange(len(segments))\n",
    "        query_indices = query_indices[query_indices > max(support_indices)]\n",
    "#if we don't have any query samples left for the file then skip it \n",
    "        if len(query_indices) == 0:\n",
    "            print(f\" No query samples left after the first 5 Positive in {file_data['file']} Skipping this file.\")\n",
    "            continue\n",
    "#extract support and query set\n",
    "        support_set = segments[support_indices]\n",
    "        query_set = segments[query_indices]\n",
    "        query_labels = labels[query_indices]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #extract features for support set\n",
    "            support_features = model.extract_features(support_set.to(device))\n",
    "            prototype = support_features.mean(dim=0)\n",
    "            \n",
    "            query_features = []\n",
    "            #extract the features for query set in batches \n",
    "            for i in range(0, len(query_set), batch_size):\n",
    "                batch = query_set[i:i+batch_size].to(device)\n",
    "                batch_features = model.extract_features(batch)\n",
    "                #move to cpu for distance calculation\n",
    "                query_features.append(batch_features.cpu())\n",
    "            query_features = torch.cat(query_features, dim=0)\n",
    "            #calculate distances between query and prototype\n",
    "\n",
    "            distances=torch.cdist(query_features, prototype.unsqueeze(0).cpu()).squeeze().numpy()\n",
    "            #then we normalise distances between 0 and 1\n",
    "            distances = (distances - distances.min()) / (distances.max() - distances.min())\n",
    "            \n",
    "            median_threshold = np.median(distances)\n",
    "            \n",
    "            best_threshold = 0\n",
    "            best_f1 = 0\n",
    "            #evaluation F metrics\n",
    "            for threshold in thresholds:\n",
    "                predictions = (distances < threshold).astype(int)\n",
    "                f1 = f1_score(query_labels, predictions, average='binary')\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_threshold = threshold\n",
    "            \n",
    "            predictions = (distances < best_threshold).astype(int)\n",
    "\n",
    "        precision = precision_score(query_labels, predictions, average='binary', zero_division=1)\n",
    "        recall = recall_score(query_labels, predictions, average='binary', zero_division=1)\n",
    "        f1 = f1_score(query_labels, predictions, average='binary')\n",
    "#store results for each file\n",
    "        all_results.append({\n",
    "            'file': file_data['file'],\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,})\n",
    "#store predictions in CSV files\n",
    "        for i, pred in enumerate(predictions):\n",
    "            if pred == 1:\n",
    "                all_predictions.append({\n",
    "                    'file': file_data['file'],\n",
    "                    'start_time': start_times[query_indices[i]],\n",
    "                    'end_time': end_times[query_indices[i]]\n",
    "                })\n",
    "\n",
    "    return all_results, all_predictions\n",
    "\n",
    "def generate_csv_output(predictions, output_file):\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        #our header /cols in csv\n",
    "        writer.writerow(['Audiofilename', 'Starttime', 'Endtime'])\n",
    "        for pred in predictions:\n",
    "            #write each prediciton as rows in the csv file\n",
    "            writer.writerow([pred['file'], pred['start_time'], pred['end_time']])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"hello\")\n",
    "    #use gpu if not available use cpu\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#train based on train_all.h5 file created in preproccessing\n",
    "    train_x, train_y = load_Training_data(os.path.join(train_path, 'train_all.h5'))\n",
    "    #after training call validation data\n",
    "    val_data = load_validation_data(val_path)\n",
    "    \n",
    "    print(f\"Training  path: {train_path}.\")\n",
    "    print(f\"Validation  path: {val_path}.\")\n",
    "    print(f\"Training shape: {train_x.shape}.\")\n",
    "\n",
    "    model = AudioResNet().to(device)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "#define loss function binary cross entropy\n",
    "    criterion = nn.BCELoss()\n",
    "    #optimiser with learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    #reduce the learning rate if loss doesn't keep decreasing after three epochs\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "    #create a pytorch dataset and loader for trainnig the data\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    train(model, train_loader, criterion, optimizer, scheduler, device, num_epochs=1, patience=2)\n",
    "#train the model and load the best model wuth lowest loss for evaluation\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "#validate using few shot learning\n",
    "    results, predictions = validate_few_shot(model, val_data, device)\n",
    "\n",
    "    \n",
    "    if results:\n",
    "        print(\"Validation Results:\")\n",
    "        avg_precision = avg_recall = avg_f1 = 0\n",
    "        for file_result in results:\n",
    "            print(f\"File: {file_result['file']}\")\n",
    "            for metric, value in file_result.items():\n",
    "                if metric != 'file' and metric != 'best_threshold':\n",
    "                    print(f\"{metric}: {value:.4f}\")\n",
    "                elif metric == 'best_threshold':\n",
    "                    print(f\"{metric}: {value:.6f}\")\n",
    "            print()\n",
    "            avg_precision += file_result['precision']\n",
    "            avg_recall += file_result['recall']\n",
    "            avg_f1 += file_result['f1']\n",
    "        \n",
    "        num_files = len(results)\n",
    "        print(\"Average Results:\")\n",
    "        print(f\"Precision: {avg_precision / num_files:.4f}\")\n",
    "        print(f\"Recall: {avg_recall / num_files:.4f}\")\n",
    "        print(f\"F1 Score: {avg_f1 / num_files:.4f}\")\n",
    "\n",
    "        generate_csv_output(predictions, 'CNN_ResNet.csv')\n",
    "        torch.save(model.state_dict(), 'resnet_model.pth')\n",
    "\n",
    "    else:\n",
    "        print(\"No Validation results  found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "be72a48a-bb5b-4557-9ef9-0faa3cffd9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pg/22n8t2m906vc18bt44y5trf80000gn/T/ipykernel_1146/2805129848.py:240: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model loaded from 'resnet_model.pth'.\n",
      "Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|                                        | 0/57 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: (b'85MGE.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'89MGE.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'CHE_04.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'CHE_05.wav',)\n",
      "Segments shape: torch.Size([18, 1, 128, 86])\n",
      "Labels shape: torch.Size([18])\n",
      "Number of positive samples: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   7%|██▏                             | 4/57 [00:00<00:06,  8.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype shape: torch.Size([1024])\n",
      "Distances shape: torch.Size([13])\n",
      "Predictions shape: torch.Size([13])\n",
      "Metrics for file (b'CHE_05.wav',):\n",
      "f1: 0.6316\n",
      "precision: 1.0000\n",
      "recall: 0.4615\n",
      "false_positive_rate: 0.0000\n",
      "false_negative_rate: 0.5385\n",
      "\n",
      "Processing file: (b'CHE_11.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'CHE_07.wav',)\n",
      "Segments shape: torch.Size([21, 1, 128, 86])\n",
      "Labels shape: torch.Size([21])\n",
      "Number of positive samples: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  11%|███▎                            | 6/57 [00:01<00:12,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype shape: torch.Size([1024])\n",
      "Distances shape: torch.Size([16])\n",
      "Predictions shape: torch.Size([16])\n",
      "Metrics for file (b'CHE_07.wav',):\n",
      "f1: 0.6087\n",
      "precision: 1.0000\n",
      "recall: 0.4375\n",
      "false_positive_rate: 0.0000\n",
      "false_negative_rate: 0.5625\n",
      "\n",
      "Processing file: (b'CHE_13.wav',)\n",
      "Segments shape: torch.Size([17, 1, 128, 86])\n",
      "Labels shape: torch.Size([17])\n",
      "Number of positive samples: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  12%|███▉                            | 7/57 [00:01<00:14,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype shape: torch.Size([1024])\n",
      "Distances shape: torch.Size([12])\n",
      "Predictions shape: torch.Size([12])\n",
      "Metrics for file (b'CHE_13.wav',):\n",
      "f1: 0.5882\n",
      "precision: 1.0000\n",
      "recall: 0.4167\n",
      "false_positive_rate: 0.0000\n",
      "false_negative_rate: 0.5833\n",
      "\n",
      "Processing file: (b'CHE_12.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'CHE_06.wav',)\n",
      "Segments shape: torch.Size([18, 1, 128, 86])\n",
      "Labels shape: torch.Size([18])\n",
      "Number of positive samples: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  16%|█████                           | 9/57 [00:02<00:12,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype shape: torch.Size([1024])\n",
      "Distances shape: torch.Size([13])\n",
      "Predictions shape: torch.Size([13])\n",
      "Metrics for file (b'CHE_06.wav',):\n",
      "f1: 0.6316\n",
      "precision: 1.0000\n",
      "recall: 0.4615\n",
      "false_positive_rate: 0.0000\n",
      "false_negative_rate: 0.5385\n",
      "\n",
      "Processing file: (b'CHE_02.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'CHE_16.wav',)\n",
      "Segments shape: torch.Size([16, 1, 128, 86])\n",
      "Labels shape: torch.Size([16])\n",
      "Number of positive samples: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  19%|█████▉                         | 11/57 [00:02<00:11,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype shape: torch.Size([1024])\n",
      "Distances shape: torch.Size([11])\n",
      "Predictions shape: torch.Size([11])\n",
      "Metrics for file (b'CHE_16.wav',):\n",
      "f1: 0.6250\n",
      "precision: 1.0000\n",
      "recall: 0.4545\n",
      "false_positive_rate: 0.0000\n",
      "false_negative_rate: 0.5455\n",
      "\n",
      "Processing file: (b'CHE_17.wav',)\n",
      "Segments shape: torch.Size([30, 1, 128, 86])\n",
      "Labels shape: torch.Size([30])\n",
      "Number of positive samples: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  21%|██████▌                        | 12/57 [00:03<00:19,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype shape: torch.Size([1024])\n",
      "Distances shape: torch.Size([25])\n",
      "Predictions shape: torch.Size([25])\n",
      "Metrics for file (b'CHE_17.wav',):\n",
      "f1: 0.6486\n",
      "precision: 1.0000\n",
      "recall: 0.4800\n",
      "false_positive_rate: 0.0000\n",
      "false_negative_rate: 0.5200\n",
      "\n",
      "Processing file: (b'91MGE.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'CHE_03.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'CHE_15.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'CHE_01.wav',)\n",
      "Segments shape: torch.Size([18, 1, 128, 86])\n",
      "Labels shape: torch.Size([18])\n",
      "Number of positive samples: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  28%|████████▋                      | 16/57 [00:04<00:10,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype shape: torch.Size([1024])\n",
      "Distances shape: torch.Size([13])\n",
      "Predictions shape: torch.Size([13])\n",
      "Metrics for file (b'CHE_01.wav',):\n",
      "f1: 0.6316\n",
      "precision: 1.0000\n",
      "recall: 0.4615\n",
      "false_positive_rate: 0.0000\n",
      "false_negative_rate: 0.5385\n",
      "\n",
      "Processing file: (b'CHE_14.wav',)\n",
      "Segments shape: torch.Size([14, 1, 128, 86])\n",
      "Labels shape: torch.Size([14])\n",
      "Number of positive samples: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  30%|█████████▏                     | 17/57 [00:04<00:11,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype shape: torch.Size([1024])\n",
      "Distances shape: torch.Size([9])\n",
      "Predictions shape: torch.Size([9])\n",
      "Metrics for file (b'CHE_14.wav',):\n",
      "f1: 0.6154\n",
      "precision: 1.0000\n",
      "recall: 0.4444\n",
      "false_positive_rate: 0.0000\n",
      "false_negative_rate: 0.5556\n",
      "\n",
      "Processing file: (b'CHE_19.wav',)\n",
      "Segments shape: torch.Size([13, 1, 128, 86])\n",
      "Labels shape: torch.Size([13])\n",
      "Number of positive samples: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  32%|█████████▊                     | 18/57 [00:04<00:11,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype shape: torch.Size([1024])\n",
      "Distances shape: torch.Size([8])\n",
      "Predictions shape: torch.Size([8])\n",
      "Metrics for file (b'CHE_19.wav',):\n",
      "f1: 0.5455\n",
      "precision: 1.0000\n",
      "recall: 0.3750\n",
      "false_positive_rate: 0.0000\n",
      "false_negative_rate: 0.6250\n",
      "\n",
      "Processing file: (b'CHE_18.wav',)\n",
      "Segments shape: torch.Size([20, 1, 128, 86])\n",
      "Labels shape: torch.Size([20])\n",
      "Number of positive samples: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  33%|██████████▎                    | 19/57 [00:05<00:12,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype shape: torch.Size([1024])\n",
      "Distances shape: torch.Size([15])\n",
      "Predictions shape: torch.Size([15])\n",
      "Metrics for file (b'CHE_18.wav',):\n",
      "f1: 0.6364\n",
      "precision: 1.0000\n",
      "recall: 0.4667\n",
      "false_positive_rate: 0.0000\n",
      "false_negative_rate: 0.5333\n",
      "\n",
      "Processing file: (b'CHE_09.wav',)\n",
      "Segments shape: torch.Size([21, 1, 128, 86])\n",
      "Labels shape: torch.Size([21])\n",
      "Number of positive samples: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  35%|██████████▉                    | 20/57 [00:06<00:16,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype shape: torch.Size([1024])\n",
      "Distances shape: torch.Size([16])\n",
      "Predictions shape: torch.Size([16])\n",
      "Metrics for file (b'CHE_09.wav',):\n",
      "f1: 0.6087\n",
      "precision: 1.0000\n",
      "recall: 0.4375\n",
      "false_positive_rate: 0.0000\n",
      "false_negative_rate: 0.5625\n",
      "\n",
      "Processing file: (b'DC01.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'DC02.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'DC12.wav',)\n",
      "Segments shape: torch.Size([32, 1, 128, 86])\n",
      "Labels shape: torch.Size([32])\n",
      "Number of positive samples: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  40%|████████████▌                  | 23/57 [00:07<00:13,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype shape: torch.Size([1024])\n",
      "Distances shape: torch.Size([27])\n",
      "Predictions shape: torch.Size([27])\n",
      "Metrics for file (b'DC12.wav',):\n",
      "f1: 0.6500\n",
      "precision: 1.0000\n",
      "recall: 0.4815\n",
      "false_positive_rate: 0.0000\n",
      "false_negative_rate: 0.5185\n",
      "\n",
      "Processing file: (b'E2_208_20190712_0150.wav',)\n",
      "Segments shape: torch.Size([2, 1, 128, 86])\n",
      "Labels shape: torch.Size([2])\n",
      "Number of positive samples: 2\n",
      "Not enough positive samples (less than 5). Skipping this file.\n",
      "\n",
      "Processing file: (b'DC06.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'DC07.wav',)\n",
      "Segments shape: torch.Size([18, 1, 128, 86])\n",
      "Labels shape: torch.Size([18])\n",
      "Number of positive samples: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  46%|██████████████▏                | 26/57 [00:07<00:09,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype shape: torch.Size([1024])\n",
      "Distances shape: torch.Size([13])\n",
      "Predictions shape: torch.Size([13])\n",
      "Metrics for file (b'DC07.wav',):\n",
      "f1: 0.6316\n",
      "precision: 1.0000\n",
      "recall: 0.4615\n",
      "false_positive_rate: 0.0000\n",
      "false_negative_rate: 0.5385\n",
      "\n",
      "Processing file: (b'DC05.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'DC11.wav',)\n",
      "Segments shape: torch.Size([27, 1, 128, 86])\n",
      "Labels shape: torch.Size([27])\n",
      "Number of positive samples: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  49%|███████████████▏               | 28/57 [00:08<00:10,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype shape: torch.Size([1024])\n",
      "Distances shape: torch.Size([22])\n",
      "Predictions shape: torch.Size([22])\n",
      "Metrics for file (b'DC11.wav',):\n",
      "f1: 0.6250\n",
      "precision: 1.0000\n",
      "recall: 0.4545\n",
      "false_positive_rate: 0.0000\n",
      "false_negative_rate: 0.5455\n",
      "\n",
      "Processing file: (b'DC10.wav',)\n",
      "Segments shape: torch.Size([35, 1, 128, 86])\n",
      "Labels shape: torch.Size([35])\n",
      "Number of positive samples: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  51%|███████████████▊               | 29/57 [00:10<00:15,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype shape: torch.Size([1024])\n",
      "Distances shape: torch.Size([30])\n",
      "Predictions shape: torch.Size([30])\n",
      "Metrics for file (b'DC10.wav',):\n",
      "f1: 0.6364\n",
      "precision: 1.0000\n",
      "recall: 0.4667\n",
      "false_positive_rate: 0.0000\n",
      "false_negative_rate: 0.5333\n",
      "\n",
      "Processing file: (b'DC04.wav',)\n",
      "Segments shape: torch.Size([2, 1, 128, 86])\n",
      "Labels shape: torch.Size([2])\n",
      "Number of positive samples: 2\n",
      "Not enough positive samples (less than 5). Skipping this file.\n",
      "\n",
      "Processing file: (b'DC08.wav',)\n",
      "Segments shape: torch.Size([22, 1, 128, 86])\n",
      "Labels shape: torch.Size([22])\n",
      "Number of positive samples: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  54%|████████████████▊              | 31/57 [00:11<00:13,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype shape: torch.Size([1024])\n",
      "Distances shape: torch.Size([17])\n",
      "Predictions shape: torch.Size([17])\n",
      "Metrics for file (b'DC08.wav',):\n",
      "f1: 0.6400\n",
      "precision: 1.0000\n",
      "recall: 0.4706\n",
      "false_positive_rate: 0.0000\n",
      "false_negative_rate: 0.5294\n",
      "\n",
      "Processing file: (b'cw1300_DCASE.wav',)\n",
      "Segments shape: torch.Size([13, 1, 128, 86])\n",
      "Labels shape: torch.Size([13])\n",
      "Number of positive samples: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  56%|█████████████████▍             | 32/57 [00:11<00:11,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype shape: torch.Size([1024])\n",
      "Distances shape: torch.Size([8])\n",
      "Predictions shape: torch.Size([8])\n",
      "Metrics for file (b'cw1300_DCASE.wav',):\n",
      "f1: 0.5455\n",
      "precision: 1.0000\n",
      "recall: 0.3750\n",
      "false_positive_rate: 0.0000\n",
      "false_negative_rate: 0.6250\n",
      "\n",
      "Processing file: (b'cw1345_DCASE.wav',)\n",
      "Segments shape: torch.Size([13, 1, 128, 86])\n",
      "Labels shape: torch.Size([13])\n",
      "Number of positive samples: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  58%|█████████████████▉             | 33/57 [00:11<00:10,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype shape: torch.Size([1024])\n",
      "Distances shape: torch.Size([8])\n",
      "Predictions shape: torch.Size([8])\n",
      "Metrics for file (b'cw1345_DCASE.wav',):\n",
      "f1: 0.5455\n",
      "precision: 1.0000\n",
      "recall: 0.3750\n",
      "false_positive_rate: 0.0000\n",
      "false_negative_rate: 0.6250\n",
      "\n",
      "Processing file: (b'cw1315_DCASE.wav',)\n",
      "Segments shape: torch.Size([23, 1, 128, 86])\n",
      "Labels shape: torch.Size([23])\n",
      "Number of positive samples: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  60%|██████████████████▍            | 34/57 [00:12<00:12,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype shape: torch.Size([1024])\n",
      "Distances shape: torch.Size([18])\n",
      "Predictions shape: torch.Size([18])\n",
      "Metrics for file (b'cw1315_DCASE.wav',):\n",
      "f1: 0.6154\n",
      "precision: 1.0000\n",
      "recall: 0.4444\n",
      "false_positive_rate: 0.0000\n",
      "false_negative_rate: 0.5556\n",
      "\n",
      "Processing file: (b'E4_49_20190804_0150.wav',)\n",
      "Segments shape: torch.Size([1, 1, 128, 86])\n",
      "Labels shape: torch.Size([1])\n",
      "Number of positive samples: 1\n",
      "Not enough positive samples (less than 5). Skipping this file.\n",
      "\n",
      "Processing file: (b'cw1330_DCASE.wav',)\n",
      "Segments shape: torch.Size([13, 1, 128, 86])\n",
      "Labels shape: torch.Size([13])\n",
      "Number of positive samples: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  63%|███████████████████▌           | 36/57 [00:12<00:08,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype shape: torch.Size([1024])\n",
      "Distances shape: torch.Size([8])\n",
      "Predictions shape: torch.Size([8])\n",
      "Metrics for file (b'cw1330_DCASE.wav',):\n",
      "f1: 0.5455\n",
      "precision: 1.0000\n",
      "recall: 0.3750\n",
      "false_positive_rate: 0.0000\n",
      "false_negative_rate: 0.6250\n",
      "\n",
      "Processing file: (b'CHE_F09.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'CHE_F08.wav',)\n",
      "Segments shape: torch.Size([16, 1, 128, 86])\n",
      "Labels shape: torch.Size([16])\n",
      "Number of positive samples: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  67%|████████████████████▋          | 38/57 [00:13<00:06,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype shape: torch.Size([1024])\n",
      "Distances shape: torch.Size([11])\n",
      "Predictions shape: torch.Size([11])\n",
      "Metrics for file (b'CHE_F08.wav',):\n",
      "f1: 0.6250\n",
      "precision: 1.0000\n",
      "recall: 0.4545\n",
      "false_positive_rate: 0.0000\n",
      "false_negative_rate: 0.5455\n",
      "\n",
      "Processing file: (b'CHE_F18.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'CHE_F19.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'CHE_F14.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'CHE_F15.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'CHE_F17.wav',)\n",
      "Segments shape: torch.Size([6, 1, 128, 86])\n",
      "Labels shape: torch.Size([6])\n",
      "Number of positive samples: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████| 57/57 [00:13<00:00,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype shape: torch.Size([1024])\n",
      "Distances shape: torch.Size([])\n",
      "Predictions shape: torch.Size([])\n",
      "Error processing file (b'CHE_F17.wav',): Singleton array array(0., dtype=float32) cannot be considered a valid collection.\n",
      "\n",
      "Processing file: (b'CHE_F03.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'CHE_F02.wav',)\n",
      "Segments shape: torch.Size([4, 1, 128, 86])\n",
      "Labels shape: torch.Size([4])\n",
      "Number of positive samples: 4\n",
      "Not enough positive samples (less than 5). Skipping this file.\n",
      "\n",
      "Processing file: (b'E1_208_20190712_0150.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'CHE_F12.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'CHE_F06.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'CHE_F07.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'CHE_F13.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'CHE_F05.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'CHE_F11.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'CHE_F10.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'ct3.wav',)\n",
      "Segments shape: torch.Size([4, 1, 128, 86])\n",
      "Labels shape: torch.Size([4])\n",
      "Number of positive samples: 4\n",
      "Not enough positive samples (less than 5). Skipping this file.\n",
      "\n",
      "Processing file: (b'ct2.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Processing file: (b'QU05.wav',)\n",
      "Segments shape: torch.Size([1, 1, 128, 86])\n",
      "Labels shape: torch.Size([1])\n",
      "Number of positive samples: 1\n",
      "Not enough positive samples (less than 5). Skipping this file.\n",
      "\n",
      "Processing file: (b'CHE_10.wav',)\n",
      "Segments shape: torch.Size([5, 1, 128, 86])\n",
      "Labels shape: torch.Size([5])\n",
      "Number of positive samples: 5\n",
      "No query samples left after selecting support set. Skipping this file.\n",
      "\n",
      "Total segments processed: 583\n",
      "Total positive segments: 583.0\n",
      "Evaluation completed. Mean Metrics:\n",
      "f1: 0.6110\n",
      "precision: 1.0000\n",
      "recall: 0.4407\n",
      "false_positive_rate: 0.0000\n",
      "false_negative_rate: 0.5593\n",
      "Results saved to evaluation_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score, precision_score, recall_sc2q121121qaore\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import csv\n",
    "\n",
    "class AudioResNet(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(AudioResNet, self).__init__()\n",
    "        self.in_channels = 128\n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.layer1 = self.make_layer(128, 2, stride=1)\n",
    "        self.layer2 = self.make_layer(256, 2, stride=2)\n",
    "        self.layer3 = self.make_layer(512, 2, stride=2)\n",
    "        self.layer4 = self.make_layer(1024, 2, stride=2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def make_layer(self, out_channels, blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(self.make_resNET_block(out_channels, stride))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(self.make_resNET_block(out_channels, stride=1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def make_resNET_block(self, out_channels, stride):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "        block = ResBlock(self.in_channels, out_channels, stride, downsample)\n",
    "        self.in_channels = out_channels\n",
    "        return block\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avg_pool(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return torch.relu(out)\n",
    "\n",
    "# AudioDataset class for loading the data\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, hdf_path):\n",
    "        self.hdf_path = hdf_path\n",
    "        with h5py.File(self.hdf_path, 'r') as hf:\n",
    "            self.keys = list(hf.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.hdf_path, 'r') as hf:\n",
    "            group = hf[self.keys[idx]]\n",
    "            segments = torch.from_numpy(group['segments'][:]).float()\n",
    "            labels = torch.from_numpy(group['labels'][:]).float()\n",
    "            start_times = torch.from_numpy(group['start_times'][:]).float()\n",
    "            end_times = torch.from_numpy(group['end_times'][:]).float()\n",
    "            file_key = group['file'][()]\n",
    "\n",
    "        if segments.ndim == 5:\n",
    "            segments = segments.squeeze(2)  # Remove the extra dimension if needed\n",
    "\n",
    "        return segments, labels, start_times, end_times, file_key\n",
    "\n",
    "\n",
    "def evaluate_file(model, segments, labels, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        print(f\"Segments shape: {segments.shape}\")\n",
    "        print(f\"Labels shape: {labels.shape}\")\n",
    "        \n",
    "        if segments.dim() == 3:\n",
    "            segments = segments.unsqueeze(1)\n",
    "        elif segments.dim() == 2:\n",
    "            segments = segments.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Check if there are at least 5 positive samples\n",
    "        pos_indices = (labels == 1).nonzero(as_tuple=True)[0]\n",
    "        print(f\"Number of positive samples: {len(pos_indices)}\")\n",
    "        \n",
    "        if len(pos_indices) < 5:\n",
    "            print(\"Not enough positive samples (less than 5). Skipping this file.\")\n",
    "            return None, None  # Skip evaluation if there are less than 5 positive samples\n",
    "\n",
    "        # Split into support and query sets\n",
    "        support_indices = pos_indices[:5]\n",
    "        query_indices = np.setdiff1d(np.arange(len(labels)), support_indices)\n",
    "\n",
    "        if len(query_indices) == 0:\n",
    "            print(\"No query samples left after selecting support set, Skipping this file\")\n",
    "            return None, None\n",
    "\n",
    "        support_set = segments[support_indices]\n",
    "        query_set = segments[query_indices]\n",
    "        query_labels = labels[query_indices]\n",
    "\n",
    "        # Extract features for support and query sets\n",
    "        support_features = model.extract_features(support_set.to(device))\n",
    "        query_features = model.extract_features(query_set.to(device))\n",
    "        \n",
    "        # Compute prototype from the support set\n",
    "        prototype = support_features.mean(dim=0)\n",
    "        print(f\"Prototype shape: {prototype.shape}\")\n",
    "        \n",
    "        # Compute distances and predictions for the query set\n",
    "        distances = torch.cdist(query_features, prototype.unsqueeze(0)).squeeze()\n",
    "        print(f\"Distances shape: {distances.shape}\")\n",
    "        \n",
    "        predictions = (distances < distances.median()).float()\n",
    "        print(f\"Predictions shape: {predictions.shape}\")\n",
    "    \n",
    "    return predictions.cpu().numpy(), query_labels.cpu().numpy()\n",
    "\n",
    "def calculate_metrics(labels, predictions):\n",
    "    metrics = {}\n",
    "    metrics['f1'] = f1_score(labels, predictions, average='binary', zero_division=0)\n",
    "    metrics['precision'] = precision_score(labels, predictions, average='binary', zero_division=0)\n",
    "    metrics['recall'] = recall_score(labels, predictions, average='binary', zero_division=0)\n",
    "    metrics['false_positive_rate'] = np.sum((predictions == 1) & (labels == 0)) / max(np.sum(labels == 0), 1)\n",
    "    metrics['false_negative_rate'] = np.sum((predictions == 0) & (labels == 1)) / max(np.sum(labels == 1), 1)\n",
    "    return metrics\n",
    "\n",
    "def evaluate(model, data_loader, device, output_file):\n",
    "    model.eval()\n",
    "    all_metrics = []\n",
    "    results = []\n",
    "    total_segments = 0\n",
    "    total_positive_segments = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for segments, labels, start_times, end_times, file_key in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            print(f\"\\nProcessing file: {file_key}\")\n",
    "            segments = segments.squeeze(0)  \n",
    "            labels = labels.squeeze(0)\n",
    "            start_times = start_times.squeeze(0)\n",
    "            end_times = end_times.squeeze(0)\n",
    "\n",
    "            total_segments += segments.shape[0]\n",
    "            total_positive_segments += torch.sum(labels).item()\n",
    "\n",
    "            try:\n",
    "                predictions, query_labels = evaluate_file(model, segments, labels, device)\n",
    "                \n",
    "                if predictions is None:\n",
    "                    continue  # Skip to the next file if evaluation was skipped\n",
    "                \n",
    "                # Compute metrics\n",
    "                file_metrics = calculate_metrics(query_labels, predictions)\n",
    "                all_metrics.append(file_metrics)\n",
    "                print(f\"Metrics for file {file_key}:\")\n",
    "                for metric, value in file_metrics.items():\n",
    "                    print(f\"{metric}: {value:.4f}\")\n",
    "                \n",
    "                for i, pred in enumerate(predictions):\n",
    "                    if pred == 1:\n",
    "                        results.append({\n",
    "                            'file': file_key[0],\n",
    "                            'start_time': start_times[i].item(),\n",
    "                            'end_time': end_times[i].item()\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_key}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    with open(output_file, 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=['file', 'start_time', 'end_time'])\n",
    "        writer.writeheader()\n",
    "        for result in results:\n",
    "            writer.writerow(result)\n",
    "\n",
    "\n",
    "\n",
    "    if all_metrics:\n",
    "        mean_metrics = {metric: np.mean([file_metric[metric] for file_metric in all_metrics]) \n",
    "                        for metric in all_metrics[0].keys()}\n",
    "        return mean_metrics\n",
    "    else:\n",
    "        return {metric: float('nan') for metric in ['f1', 'precision', 'recall', 'false_positive_rate', 'false_negative_rate']}\n",
    "\n",
    "\n",
    "def main():\n",
    "    hdf_eval = \"/Users/zaher/Desktop/Project/eval_2/eval_all.h5\"\n",
    "    output_file = \"evaluation_results.csv\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    eval_dataset = AudioDataset(hdf_eval)\n",
    "    eval_loader = DataLoader(eval_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model = AudioResNet().to(device)\n",
    "    model_path = 'resnet_model.pth'\n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        print(f\"Model loaded sucessfully from '{model_path}'.\")\n",
    "    else:\n",
    "        print(f\"Error: Model file '{model_path}' not found.\")\n",
    "        return\n",
    "\n",
    "    print(\"Starting Evaluation Process\")\n",
    "    try:\n",
    "        mean_metrics = evaluate(model, eval_loader, device, output_file)\n",
    "        print(\"Evaluation Process completed. Mean Metrics:\")\n",
    "        for metric, value in mean_metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        print(f\"Results saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during evaluation process: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
